# GitHub Issues Analysis Tools

Tools to analyze Slang's GitHub issues and pull requests for identifying quality gaps, bug-prone components, and areas needing improvement.

## Purpose

These scripts help answer questions like:
- Which components have the most bugs?
- What files are changed most often for bug fixes?
- Where should we focus testing efforts?
- Which areas take longest to fix (complexity indicators)?
- What's the bug fix rate and test coverage?
- Are there patterns in crashes, ICEs, or validation errors?

## Available Scripts

### 1. `fetch_github_issues.py`
Downloads issue and PR data from GitHub API with full historical data.

**Features:**
- Fetches all open and closed issues and PRs
- Extracts issue references from PR titles/bodies (e.g., "fixes #123")
- Downloads file changes for each PR
- Supports incremental updates (only fetch new/updated items)

### 2. `analyze_issues.py`
General quality analysis across all issues and PRs.

**Reports on:**
- Bug rate: 16.2% of issues, 35.5% of PRs
- Component-level bug distribution
- Time to close by component
- Test coverage by PR type
- Most frequently changed files
- Development velocity

### 3. `analyze_critical_issues.py`
Deep dive into critical issues (crashes, ICEs, validation errors).

**Reports on:**
- Critical issue categorization
- Root cause component analysis
- File-level hotspots for critical bugs
- Open critical issues ranked by urgency

### 4. `analyze_bugfix_files.py`
File-level bug fix analysis.

**Reports on:**
- Most bug-prone files and components
- Source file breakdown by component (30+ categories)
- Bug fix frequency by file type
- Top files per component

## Quick Start

### Step 1: Fetch Data

```bash
cd tools/issue-analysis

# Optional: Set GitHub token to avoid rate limits
export GITHUB_TOKEN="your_github_token_here"

# Full fetch (~15-20 minutes, first time)
python3 fetch_github_issues.py

# Incremental update (~30 seconds, subsequent runs)
python3 fetch_github_issues.py --incremental
```

**Note**: Without a GitHub token, you may hit rate limits (60 requests/hour). With a token, you get 5000 requests/hour. Create a token at: https://github.com/settings/tokens (only needs public repo read access)

### Step 2: Run Analysis

```bash
# General analysis (bugs, components, files, coverage)
python3 analyze_issues.py

# Critical issues (crashes, ICEs, validation errors)
python3 analyze_critical_issues.py

# Bug-fix file hotspots
python3 analyze_bugfix_files.py
```

## Data Format

### Fetched Data

After running `fetch_github_issues.py`, you'll have:

```
tools/issue-analysis/data/
├── issues.json           # All issues with full metadata
├── pull_requests.json    # All PRs with files changed and issue references
├── metadata.json         # Fetch timestamp and enrichment info
└── issues_detailed.csv   # Generated by analyze_issues.py
```

### PR Data Structure

Each PR includes:

```json
{
  "number": 8999,
  "title": "Fix SPIRV emission bug",
  "state": "closed",
  "referenced_issues": [123, 456],  // Extracted from title/body
  "files_changed": [
    {
      "filename": "source/slang/slang-emit-spirv.cpp",
      "status": "modified",
      "additions": 25,
      "deletions": 10,
      "changes": 35
    }
  ]
}
```

## Incremental Updates

The `--incremental` flag makes subsequent fetches much faster:

```bash
# After initial full fetch, use incremental for updates
python3 fetch_github_issues.py --incremental
```

**How it works:**
- Uses GitHub's `since` parameter to fetch only items updated after last fetch
- Automatically enriches only new/updated PRs (skips already-enriched data)
- Typically <100 API calls for weekly updates vs ~5,500 for full fetch

**When to use:**
- ✅ Daily/weekly updates
- ✅ After checking out fresh code (if data exists)
- ❌ First time setup (no existing data)

## Analysis Output

### Console Reports

Each analysis script prints a comprehensive report to the console with sections like:

```
TOP 15 COMPONENTS BY ISSUE COUNT (Quality Gap Indicators)
----------------------------------------------------------------------
spirv                      827 issues  (233 bugs,  54 open bugs)
hlsl                       703 issues  (120 bugs,  18 open bugs)
glsl                       661 issues  (144 bugs,  29 open bugs)
...

BUGS BY COMPONENT (Areas needing attention)
----------------------------------------------------------------------
spirv                      233 bugs total  ( 54 open, 179 closed)
glsl                       144 bugs total  ( 29 open, 115 closed)
hlsl                       120 bugs total  ( 18 open, 102 closed)
...

TOP 20 MOST FREQUENTLY CHANGED FILES (Quality Hot Spots)
----------------------------------------------------------------------
548x  source/slang/hlsl.meta.slang
505x  source/slang/slang-lower-to-ir.cpp
502x  source/slang/slang.cpp
...
```

### CSV Export

`analyze_issues.py` exports `data/issues_detailed.csv` which can be imported into:
- Excel/Google Sheets for pivot tables and charts
- Jupyter Notebook for custom Python analysis
- SQL Database for complex queries
- Pandas for data science workflows

## Component Categories

The analysis automatically categorizes issues/PRs into 30+ components:

**Compiler Pipeline:**
- `semantic-check` - Type checking and validation
- `parser` - Source code parsing
- `ir-generation` - AST to IR lowering
- `ir-passes` - IR transformations
- `compiler-core` - Core infrastructure

**Code Generation:**
- `spirv-emit`, `hlsl-emit`, `glsl-emit`, `cuda-emit`, `metal-emit`, `dxil-emit`
- `emit-common` - Shared code generation logic

**Standard Libraries:**
- `hlsl-stdlib`, `core-stdlib`, `stdlib`

**Special Features:**
- `ir-autodiff` - Automatic differentiation
- `ir-legalization` - Type and layout legalization
- `ir-specialization` - Generic specialization
- `reflection` - Parameter binding and reflection

**Infrastructure:**
- `test` - Test infrastructure
- `build-system` - CMake, CI/CD
- `gfx-rhi` - Graphics RHI layer
- `docs` - Documentation

## Customization

### Add New Component Patterns

Edit `analyze_issues.py` or `analyze_bugfix_files.py`:

```python
def extract_keywords(text):
    # Add patterns here
    patterns = {
        'my-component': [r'\bmy-keyword\b', r'\bother-keyword\b'],
        ...
    }
```

### Change Bug Detection

Edit `analyze_issues.py`:

```python
def is_bug_fix(item):
    # Customize bug detection logic
    # Currently looks for: labels, title patterns, critical keywords
```

### Export Different Formats

Modify `print_report()` functions to export JSON, HTML, or other formats.

## Workflow Examples

### Weekly Quality Review

```bash
# Update data (fast)
python3 fetch_github_issues.py --incremental

# Generate reports
python3 analyze_issues.py > weekly-report.txt
python3 analyze_critical_issues.py > critical-issues.txt
```

## Tips

1. **First run**: Use full fetch without `--incremental` (~15-20 minutes)
2. **Regular updates**: Use `--incremental` for speed (~30 seconds)
3. **Rate limits**: Set `GITHUB_TOKEN` to avoid hitting API limits
4. **Large output**: Pipe to file or `less` for easier browsing
5. **CSV analysis**: Import `issues_detailed.csv` into Excel for custom pivots

## Requirements

- Python 3.6+
- No external dependencies (uses standard library only)
- GitHub token recommended (not required)

## Troubleshooting

**SSL Certificate Error:**
```bash
# macOS: Install certificates
pip3 install --upgrade certifi
```

**Rate Limit Error:**
- Set `GITHUB_TOKEN` environment variable
- Wait for rate limit reset (shown in error message)
- Use `--incremental` for subsequent runs

**Missing Data:**
- Run `fetch_github_issues.py` first to download data
- Check `data/metadata.json` for fetch timestamp
